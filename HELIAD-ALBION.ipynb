{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df=pd.read_csv('albion_on_paper_and_metabs_30-3_4.csv')\n",
    "\n",
    "df = df.replace({',': '.'}, regex=True)\n",
    "\n",
    "def convert_to_float(val):\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return val  # Return the value as-is if it can't be converted to float\n",
    "\n",
    "# Apply the function element-wise to the DataFrame\n",
    "df = df.applymap(convert_to_float)\n",
    "\n",
    "object_columns = df.select_dtypes(include='object').columns\n",
    "print(object_columns)\n",
    "\n",
    "\n",
    "X=df.drop(columns=['DIAGNOSIS'])\n",
    "y=df['DIAGNOSIS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1066)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "print(\"classifier finished\")\n",
    "\n",
    "\n",
    "    \n",
    "#print accuracy\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "from model import Selection\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split,RepeatedKFold,StratifiedShuffleSplit\n",
    "\n",
    "from collections import Counter\n",
    "Counts=Counter()\n",
    "X_train_scaled=pd.DataFrame(X_train_scaled)\n",
    "X_test_scaled=pd.DataFrame(X_test_scaled)\n",
    "for i in range(0,10000):\n",
    "    metas=list(X_train_scaled.columns)\n",
    "\n",
    "    Xtrain=X_train_scaled.values\n",
    "    Xtest=X_test_scaled.values\n",
    "    class ArgsClass:\n",
    "        def __init__(self,k=5,alpha=0.05):\n",
    "            self.k=k \n",
    "            self.alpha=alpha\n",
    "    args=ArgsClass(k=30,alpha=0.005)\n",
    "    sel=Selection(\"lasso\",args)\n",
    "    sel.fit(Xtrain,y_train)\n",
    "    mb=sel.mb_\n",
    "    Counts=Counter(mb)+Counts\n",
    "\n",
    "Counts_metas=pd.DataFrame(Counts.most_common(),columns=['metas_index','counts'])\n",
    "Counts_metas['metas'] = Counts_metas.apply(lambda a:metas[a['metas_index']],axis=1)\n",
    "selected_feature_index=list(Counts_metas['metas_index'][0:10])\n",
    "print(selected_feature_index)\n",
    "\n",
    "s=[]\n",
    "s1=[]\n",
    "model = RF(n_estimators=100,random_state=12306)\n",
    "model.fit(Xtrain[:,Counts_metas['metas_index'][0:10]], y_train)\n",
    "pred = model.predict(Xtest[:,Counts_metas['metas_index'][0:10]])\n",
    "prob = model.predict_proba(Xtest[:,Counts_metas['metas_index'][0:10]])\n",
    "print(\"ytest,pred:\",y_test,pred)\n",
    "s.append(f1_score(y_test, pred))\n",
    "print(\"ytest,prob:\",y_test,prob[:,-1])\n",
    "s1.append(roc_auc_score(y_test, prob[:,-1]))\n",
    "print(\"f1 score:\", s)\n",
    "print(\"auroc score:\", s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df.columns[selected_feature_index].tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "df2=pd.read_csv('heliad_on_paper_and_metabs_2.csv')\n",
    "X=df2.drop(columns=['G21'])\n",
    "y=df2['G21']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1066)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# Perform 10-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(rf, X, y, cv=cv, scoring='roc_auc')\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Fit model on entire dataset for feature importance\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': [f'Feature {i}' for i in range(X.shape[1])],\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort by importance and print top 20\n",
    "top_20 = importance_df.sort_values(by='Importance', ascending=False).head(20)\n",
    "print(top_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df2=pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[614,615,14,616,492,376,359,594,408,404,365,23,415,596,457,349,318,8,31,505]\n",
    "column_names = df2.columns[indices].to_list()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[620,619,618,378,448,274,72,433,244,340,459,409,256,473,10,493,111,485,466,335]\n",
    "column_names = X.columns[indices].tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('heliad_on_paper_and_metabs_2.csv')\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('albion_on_paper_and_metabs_30-3_4.csv')\n",
    "df1=df1.drop(columns=['SCC12','HADS_D1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "common_columns = []\n",
    "for col1 in df1.columns:\n",
    "    for col2 in df2.columns:\n",
    "        if fuzz.ratio(col1.lower(), col2.lower()) >= 85:  # Case-insensitive comparison\n",
    "            common_columns.append((col1, col2))\n",
    "\n",
    "# Print matched columns\n",
    "print(\"Matching columns (DF1 -> DF2):\")\n",
    "for col1, col2 in common_columns:\n",
    "    print(f\"{col1} -> {col2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = []\n",
    "for col1 in df1.columns:\n",
    "    for col2 in df2.columns:\n",
    "        if fuzz.ratio(col1.lower(), col2.lower()) >= 95:  # Case-insensitive comparison\n",
    "            common_columns.append((col1, col2))\n",
    "\n",
    "# Print matched columns\n",
    "print(\"Matching columns (DF1 -> DF2):\")\n",
    "for col1, col2 in common_columns:\n",
    "    print(f\"{col1} -> {col2}\")\n",
    "\n",
    "# Print total count\n",
    "print(f\"Total matched columns: {len(common_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_cols_df1 = set()\n",
    "matched_cols_df2 = set()\n",
    "common_columns = []\n",
    "\n",
    "for col1 in df1.columns:\n",
    "    for col2 in df2.columns:\n",
    "        if fuzz.ratio(col1.lower(), col2.lower()) >= 99:  # Case-insensitive comparison\n",
    "            common_columns.append((col1, col2))\n",
    "            matched_cols_df1.add(col1)\n",
    "            matched_cols_df2.add(col2)\n",
    "\n",
    "# Unmatched columns\n",
    "unmatched_cols_df1 = set(df1.columns) - matched_cols_df1\n",
    "unmatched_cols_df2 = set(df2.columns) - matched_cols_df2\n",
    "\n",
    "# Print results\n",
    "print(\"Matching columns (DF1 -> DF2):\")\n",
    "for col1, col2 in common_columns:\n",
    "    print(f\"{col1} -> {col2}\")\n",
    "\n",
    "print(f\"\\nTotal matched columns: {len(common_columns)}\")\n",
    "\n",
    "print(\"\\nUnmatched columns in DF1:\", unmatched_cols_df1)\n",
    "print(\"Unmatched columns in DF2:\", unmatched_cols_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_cols_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.replace({',': '.'}, regex=True)\n",
    "\n",
    "def convert_to_float(val):\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return val  # Return the value as-is if it can't be converted to float\n",
    "\n",
    "# Apply the function element-wise to the DataFrame\n",
    "df1 = df1.applymap(convert_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.replace({',': '.'}, regex=True)\n",
    "\n",
    "def convert_to_float(val):\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return val  # Return the value as-is if it can't be converted to float\n",
    "\n",
    "# Apply the function element-wise to the DataFrame\n",
    "df2 = df2.applymap(convert_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.drop(columns=['F25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('albion_31-3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('heliad_31-3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['C0'].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.read_csv('heliad_baseline.csv')\n",
    "df3['A1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=pd.read_csv('heliadteliko.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Client CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df3[df3['A1'].isin(df4['Client CODE'])][['A1', 'minimental30']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['minimental30'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['MMSE']=[23.0,\n",
    " 29.0,\n",
    " 21.0,\n",
    " 21.0,\n",
    " 28.0,\n",
    " 28.0,\n",
    " np.nan,\n",
    " np.nan,\n",
    " 29.0,\n",
    " 27.0,\n",
    " 26.0,\n",
    " 28.0,\n",
    " 25.0,\n",
    " 22.0,\n",
    " 27.0,\n",
    " np.nan,\n",
    " 27.0,\n",
    " 27.0,\n",
    " 29.0,\n",
    " 20.0,\n",
    " 25.0,\n",
    " 23.0,\n",
    " 26.0,\n",
    " 27.0,\n",
    " 23.0,\n",
    " 23.0,\n",
    " 29.0,\n",
    " 29.0,\n",
    " np.nan,\n",
    " 19.0,\n",
    " 26.0,\n",
    " 15.0,\n",
    " 29.0,\n",
    " 24.0,\n",
    " 27.0,\n",
    " 27.0,\n",
    " 29.0,\n",
    " 28.0,\n",
    " 27.0,\n",
    " 28.0,\n",
    " 25.0,\n",
    " 22.0,\n",
    " 29.0,\n",
    " 25.0,\n",
    " 24.0,\n",
    " 27.0,\n",
    " 28.0,\n",
    " 24.0,\n",
    " 27.0,\n",
    " 24.0,\n",
    " 25.0,\n",
    " 23.0,\n",
    " 26.0,\n",
    " np.nan,\n",
    " 21.0,\n",
    " 27.0,\n",
    " 27.0,\n",
    " 30.0,\n",
    " 28.0,\n",
    " 28.0,\n",
    " 23.0,\n",
    " 27.0,\n",
    " 27.0,\n",
    " 28.0,\n",
    " 29.0,\n",
    " np.nan,\n",
    " 26.0,\n",
    " 28.0,\n",
    " 24.0,\n",
    " 24.0,\n",
    " 27.0,\n",
    " 19.0,\n",
    " 30.0,\n",
    " 23.0,\n",
    " 16.0,\n",
    " 27.0,\n",
    " 25.0,\n",
    " np.nan,\n",
    " 27.0,\n",
    " 30.0,\n",
    " np.nan,\n",
    " np.nan,\n",
    " 26.0,\n",
    " np.nan,\n",
    " 27.0,\n",
    " 25.0,\n",
    " np.nan,\n",
    " 26.0,\n",
    " 29.0,\n",
    " np.nan,\n",
    " 26.0,\n",
    " 30.0,\n",
    " np.nan,\n",
    " 28.0,\n",
    " 30.0,\n",
    " 29.0,\n",
    " 29.0,\n",
    " np.nan,\n",
    " 28.0,\n",
    " 29.0,\n",
    " 29.0,\n",
    " 28.0,\n",
    " 30.0,\n",
    " 30.0,\n",
    " 30.0,\n",
    " np.nan,\n",
    " 26.0,\n",
    " 26.0,\n",
    " np.nan,\n",
    " 27.0,\n",
    " 29.0,\n",
    " np.nan,\n",
    " 24.0,\n",
    " np.nan,\n",
    " 29.0,\n",
    " 30.0,\n",
    " 30.0,\n",
    " 28.0,\n",
    " 30.0,\n",
    " 27.0,\n",
    " 25.0,\n",
    " np.nan,\n",
    " 30.0,\n",
    " 25.0,\n",
    " 28.0,\n",
    " np.nan,\n",
    " np.nan,\n",
    " 27.0,\n",
    " np.nan,\n",
    " 30.0,\n",
    " 22.0,\n",
    " 24.0,\n",
    " 30.0,\n",
    " np.nan,\n",
    " 26.0,\n",
    " np.nan,\n",
    " 27.0,\n",
    " 29.0,\n",
    " np.nan,\n",
    " 21.0,\n",
    " 29.0,\n",
    " 29.0,\n",
    " 27.0,\n",
    " np.nan,\n",
    " 27.0,\n",
    " np.nan,\n",
    " 28.0,\n",
    " 29.0,\n",
    " 29.0,\n",
    " 23.0,\n",
    " 27.0,\n",
    " 27.0,\n",
    " 28.0,\n",
    " 24.0,\n",
    " 29.0,\n",
    " 29.0,\n",
    " 27.0,\n",
    " 30.0,\n",
    " 21.0,\n",
    " 29.0,\n",
    " 17.0,\n",
    " 22.0,\n",
    " 26.0,\n",
    " 27.0,\n",
    " 28.0,\n",
    " 29.0,\n",
    " 27.0,\n",
    " 25.0,\n",
    " 26.0,\n",
    " 29.0,\n",
    " 27.0,\n",
    " 21.0,\n",
    " np.nan,\n",
    " 28.0,\n",
    " 22.0,\n",
    " 30.0,\n",
    " 26.0,\n",
    " 24.0,\n",
    " 28.0,\n",
    " 29.0,\n",
    " 23.0,\n",
    " 22.0,\n",
    " 28.0,\n",
    " 29.0,\n",
    " 25.0,\n",
    " 25.0,\n",
    " np.nan,\n",
    " np.nan,\n",
    " 28.0,\n",
    " 29.0,\n",
    " 28.0,\n",
    " 27.0,\n",
    " 21.0,\n",
    " 26.0,\n",
    " 24.0,\n",
    " 23.0,\n",
    " 30.0,\n",
    " 26.0,\n",
    " 29.0,\n",
    " 21.0,\n",
    " 27.0,\n",
    " 27.0,\n",
    " np.nan,\n",
    " 20.0,\n",
    " np.nan,\n",
    " 27.0,\n",
    " np.nan,\n",
    " np.nan,\n",
    " 29.0,\n",
    " 28.0,\n",
    " np.nan,\n",
    " 29.0,\n",
    " 28.0,\n",
    " np.nan,\n",
    " 30.0,\n",
    " 24.0,\n",
    " np.nan,\n",
    " 25.0,\n",
    " np.nan,\n",
    " np.nan,\n",
    " np.nan,\n",
    " np.nan,\n",
    " np.nan,\n",
    " 17.0,\n",
    " 23.0,\n",
    " 27.0,\n",
    " 24.0,\n",
    " 26.0,\n",
    " 29.0,\n",
    " 29.0,\n",
    " 28.0,\n",
    " 26.0,\n",
    " 26.0,\n",
    " 21.0,\n",
    " 24.0,\n",
    " 27.0,\n",
    " 24.0,\n",
    " 27.0,\n",
    " 22.0,\n",
    " np.nan,\n",
    " 28.0,\n",
    " np.nan,\n",
    " 19.0,\n",
    " 21.0,\n",
    " 24.0,\n",
    " np.nan,\n",
    " 23.0,\n",
    " 22.0,\n",
    " 28.0,\n",
    " np.nan,\n",
    " 28.0,\n",
    " 25.0,\n",
    " 29.0,\n",
    " 24.0,\n",
    " 29.0,\n",
    " 23.0,\n",
    " 22.0,\n",
    " 23.0,\n",
    " 27.0,\n",
    " 27.0,\n",
    " 26.0,\n",
    " 25.0,\n",
    " np.nan,\n",
    " 24.0,\n",
    " 26.0,\n",
    " 25.0,\n",
    " 24.0,\n",
    " 25.0,\n",
    " 22.0,\n",
    " np.nan,\n",
    " 27.0,\n",
    " 27.0,\n",
    " 25.0,\n",
    " 26.0,\n",
    " 27.0,\n",
    " 29.0,\n",
    " 24.0,\n",
    " 27.0,\n",
    " 27.0,\n",
    " 19.0,\n",
    " 19.0,\n",
    " 26.0,\n",
    " np.nan,\n",
    " 12.0,\n",
    " 28.0,\n",
    " 25.0,\n",
    " 26.0,\n",
    " 15.0,\n",
    " 21.0,\n",
    " 29.0,\n",
    " 23.0,\n",
    " 19.0,\n",
    " 25.0,\n",
    " np.nan,\n",
    " 27.0,\n",
    " 29.0,\n",
    " 24.0,\n",
    " 26.0,\n",
    " 27.0,\n",
    " 26.0,\n",
    " 23.0,\n",
    " 25.0,\n",
    " 26.0,\n",
    " 30.0,\n",
    " 27.0,\n",
    " np.nan,\n",
    " 29.0,\n",
    " 24.0,\n",
    " 17.0,\n",
    " 28.0,\n",
    " 27.0,\n",
    " 26.0,\n",
    " 22.0,\n",
    " 21.0,\n",
    " 28.0,\n",
    " 18.0,\n",
    " np.nan,\n",
    " 27.0,\n",
    " 28.0,\n",
    " 17.0,\n",
    " 29.0,\n",
    " 26.0,\n",
    " 24.0,\n",
    " 24.0,\n",
    " 27.0,\n",
    " 24.0,\n",
    " 24.0,\n",
    " 26.0,\n",
    " 30.0,\n",
    " 24.0,\n",
    " 29.0,\n",
    " 27.0,\n",
    " 28.0,\n",
    " 27.0,\n",
    " 22.0,\n",
    " 29.0,\n",
    " 21.0,\n",
    " 30.0,\n",
    " 26.0,\n",
    " 21.0,\n",
    " 29.0,\n",
    " 25.0,\n",
    " 28.0,\n",
    " 27.0,\n",
    " 22.0,\n",
    " 27.0,\n",
    " 27.0,\n",
    " 26.0,\n",
    " 19.0,\n",
    " 20.0,\n",
    " 27.0,\n",
    " 27.0,\n",
    " 24.0,\n",
    " 23.0,\n",
    " np.nan,\n",
    " 21.0,\n",
    " 22.0,\n",
    " 29.0,\n",
    " 25.0,\n",
    " 24.0,\n",
    " 21.0,\n",
    " 17.0,\n",
    " 25.0,\n",
    " 26.0,\n",
    " 26.0,\n",
    " 22.0,\n",
    " 23.0,\n",
    " 26.0,\n",
    " 27.0,\n",
    " 27.0,\n",
    " 23.0,\n",
    " 25.0,\n",
    " 29.0,\n",
    " 24.0,\n",
    " 21.0,\n",
    " 28.0,\n",
    " 26.0,\n",
    " 29.0,\n",
    " 27.0,\n",
    " 24.0,\n",
    " np.nan,\n",
    " 21.0,\n",
    " 21.0,\n",
    " 20.0,\n",
    " 26.0,\n",
    " 24.0,\n",
    " 26.0,\n",
    " 25.0,\n",
    " 27.0,\n",
    " 22.0,\n",
    " 28.0,\n",
    " 20.0,\n",
    " 26.0,\n",
    " 26.0,\n",
    " 20.0,\n",
    " 26.0,\n",
    " 27.0,\n",
    " 25.0,\n",
    " 22.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('heliad_31-3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['G21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # Enables IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate IterativeImputer with RandomForestRegressor as the estimator\n",
    "imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "                           max_iter=20, random_state=42)\n",
    "\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df2), columns=df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['MMSE'].fillna(df2['MMSE'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['MMSE'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('heliad_31-3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "df=pd.read_csv('heliad_31-3.csv')\n",
    "X=df.drop(columns=['G21'])\n",
    "y=df['G21']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111112,stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "# Initialize logistic regression with strong regularization (small C value)\n",
    "log_reg = LogisticRegression(C=0.1, solver='saga', max_iter=20000,penalty='l1')\n",
    "\n",
    "log_reg.fit(X_train_scaled,y_train)\n",
    "y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]  # Get probabilities for the positive class (index 1)\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Print the ROC AUC score\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(log_reg.predict_proba, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# Step 6: Visualize SHAP results\n",
    "shap.summary_plot(shap_values[:,:,1], X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rf_model)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[:,:,1], X_test_scaled,max_display=50,feature_names=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [489, 186, 358,364,375,546,420,613,614,505,14,356,52,457,330,44,360,615]  # Example indices\n",
    "column_names = X.columns[indices].tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [618, 619, 620,131,187,282,256,610,15,247,72,341,556,293,188,274,441]  # Example indices\n",
    "column_names = X.columns[indices].tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=300, learning_rate=0.1, random_seed=123)\n",
    "model.fit(X_train_scaled, y_train, verbose=False, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = model.get_feature_importance(Pool(X, y), type='ShapValues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_value = shap_values[0,-1]\n",
    "shap_values = shap_values[:,:-1]\n",
    "\n",
    "# visualize the first prediction's explanation\n",
    "shap.force_plot(expected_value, shap_values[0,:], X.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('heliad_31-3.csv')\n",
    "X=df.drop(columns=['G21'])\n",
    "y=df['G21']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (important for some models like Random Forests)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit scaler on training data and transform\n",
    "X_test_scaled = scaler.transform(X_test)  # Use the already fitted scaler to transform test data\n",
    "\n",
    "# Initialize Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Initialize StratifiedKFold for 10-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store the ROC AUC scores for each fold\n",
    "roc_auc_scores = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_scaled, y_train), 1):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Train the model on the current fold's training data\n",
    "    rf_classifier.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict probabilities on the validation set\n",
    "    y_prob = rf_classifier.predict_proba(X_val_fold)[:, 1]  # Probabilities for class 1\n",
    "    \n",
    "    # Calculate the ROC AUC score for the current fold\n",
    "    roc_auc = roc_auc_score(y_val_fold, y_prob)\n",
    "    \n",
    "    # Append the ROC AUC score for this fold to the list\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    \n",
    "    # Print the ROC AUC score for the current fold\n",
    "    print(f\"Fold {fold} ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Calculate and print the mean ROC AUC score after the cross-validation ends\n",
    "mean_roc_auc = np.mean(roc_auc_scores)\n",
    "print(f\"\\nMean ROC AUC across all folds: {mean_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(rf_classifier, X_test_scaled, y_test, scoring='roc_auc', n_repeats=10, random_state=42)\n",
    "\n",
    "# Extract feature importances and sort them in descending order\n",
    "sorted_idx = np.argsort(perm_importance.importances_mean)[::-1]\n",
    "\n",
    "# Print the top 10 most important features\n",
    "print(\"\\nTop 10 Feature Importances (Permutation Importance):\")\n",
    "for i in range(min(10, len(sorted_idx))):  # Print up to the top 10 features\n",
    "    feature_index = sorted_idx[i]\n",
    "    print(f\"Feature {feature_index+1}: Importance = {perm_importance.importances_mean[feature_index]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [614, 615, 586,251,285,392,565,308,126,534]  # Example indices\n",
    "column_names = X.columns[indices].tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('albion_metab_and_diagnosis.csv')\n",
    "X=df.drop(columns=['DIAGNOSIS'])\n",
    "y=df['DIAGNOSIS']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (important for some models like Random Forests)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit scaler on training data and transform\n",
    "X_test_scaled = scaler.transform(X_test)  # Use the already fitted scaler to transform test data\n",
    "\n",
    "# Initialize Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42,min_samples_split=5,max_depth=6)\n",
    "\n",
    "# Initialize StratifiedKFold for 10-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store the ROC AUC scores for each fold\n",
    "roc_auc_scores = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_scaled, y_train), 1):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Train the model on the current fold's training data\n",
    "    rf_classifier.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict probabilities on the validation set\n",
    "    y_prob = rf_classifier.predict_proba(X_val_fold)[:, 1]  # Probabilities for class 1\n",
    "    \n",
    "    # Calculate the ROC AUC score for the current fold\n",
    "    roc_auc = roc_auc_score(y_val_fold, y_prob)\n",
    "    \n",
    "    # Append the ROC AUC score for this fold to the list\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    \n",
    "    # Print the ROC AUC score for the current fold\n",
    "    print(f\"Fold {fold} ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Calculate and print the mean ROC AUC score after the cross-validation ends\n",
    "mean_roc_auc = np.mean(roc_auc_scores)\n",
    "print(f\"\\nMean ROC AUC across all folds: {mean_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(rf_classifier, X_test_scaled, y_test, scoring='roc_auc', n_repeats=10, random_state=42)\n",
    "\n",
    "# Extract feature importances and sort them in descending order\n",
    "sorted_idx = np.argsort(perm_importance.importances_mean)[::-1]\n",
    "\n",
    "# Print the top 10 most important features\n",
    "print(\"\\nTop 10 Feature Importances (Permutation Importance):\")\n",
    "for i in range(min(20, len(sorted_idx))):  # Print up to the top 10 features\n",
    "    feature_index = sorted_idx[i]\n",
    "    print(f\"Feature {feature_index+1}: Importance = {perm_importance.importances_mean[feature_index]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [341,456,28,75,334,19,383,133,606,402]  # Example indices\n",
    "column_names = X.columns[indices].tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('heliad_31-3.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "X=df.drop('G21',axis=1)\n",
    "y=df['G21']\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss=StandardScaler()\n",
    "X_trains=ss.fit_transform(X_train)\n",
    "X_tests=ss.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc=SVC()\n",
    "svc.fit(X_trains,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loocv procedure\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from numpy import mean,std\n",
    "import pandas as pd\n",
    "df=pd.read_csv('heliad_31-3.csv')\n",
    "X=df.drop(columns=['G21'])\n",
    "y=df['G21']\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss=StandardScaler()\n",
    "X=ss.fit_transform(X)\n",
    "cv = LeaveOneOut()\n",
    "# create model\n",
    "model = RandomForestClassifier(n_estimators=200,max_depth=5,random_state=12)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=GradientBoostingClassifier(), n_features_to_select=20)\n",
    "model = GradientBoostingClassifier()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0,test_size=0.25)\n",
    "\n",
    "pipe = Pipeline([('Feature Selection', rfe), ('Model', model)])\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=36851234)\n",
    "n_scores = cross_val_score(pipe, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "np.mean(n_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['G21'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('albion_on_paper_and_metabs_30-3_4.csv')\n",
    "df2['DIAGNOSIS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('albmet_MCI_Imp_INT.csv')\n",
    "df.shape\n",
    "df=df.drop(columns=['Sample','Client CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
    "\n",
    "X=df.drop(columns=['albMCI'])\n",
    "y=df['albMCI']\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=42,min_samples_split=2,max_depth=8)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "# Initialize StratifiedKFold for 10-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store the ROC AUC scores for each fold\n",
    "roc_auc_scores = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Train the model on the current fold's training data\n",
    "    rf_classifier.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict probabilities on the validation set\n",
    "    y_prob = rf_classifier.predict_proba(X_val_fold)[:, 1]  # Probabilities for class 1\n",
    "    \n",
    "    # Calculate the ROC AUC score for the current fold\n",
    "    roc_auc = roc_auc_score(y_val_fold, y_prob)\n",
    "    \n",
    "    # Append the ROC AUC score for this fold to the list\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    \n",
    "    # Print the ROC AUC score for the current fold\n",
    "    print(f\"Fold {fold} ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Calculate and print the mean ROC AUC score after the cross-validation ends\n",
    "mean_roc_auc = np.mean(roc_auc_scores)\n",
    "print(f\"\\nMean ROC AUC across all folds: {mean_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, make_scorer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit, StratifiedKFold\n",
    "import re\n",
    "\n",
    "import pathlib\n",
    "rf_estimator = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=123)\n",
    "important_features = set()\n",
    "important_features_size = 50\n",
    "for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "    rf_estimator.fit(X.iloc[train], y[train])\n",
    "    y_predicted = rf_estimator.predict(X.iloc[test])\n",
    "    print (classification_report(y[test], y_predicted))\n",
    "    \n",
    "    # print important features\n",
    "    # model important feature\n",
    "    fea_importance = rf_estimator.feature_importances_\n",
    "    indices = np.argsort(fea_importance)[::-1]\n",
    "    for f in range(important_features_size):\n",
    "        # print(\"%d. feature: %s (%f)\" % (f + 1, X_full.columns.values[indices[f]], fea_importance[indices[f]]))\n",
    "        important_features.add(X.columns.values[indices[f]])\n",
    "    #lime interpretability \n",
    "    '''explainer = lime.lime_tabular.LimeTabularExplainer(np.array(X_full_imput[train]), \n",
    "                                                       feature_names=[change_feature_names(fea) for fea in X_full.columns.values], \n",
    "                                                       class_names=rf_estimator.classes_, discretize_continuous=True, random_state=123)\n",
    "    exp = explainer.explain_instance(X_full_imput[test][5], rf_estimator.predict_proba, num_features=20)\n",
    "    exp.show_in_notebook(show_table=True, show_all=False)'''\n",
    "    #print (exp.as_list())\n",
    "    #fig = exp.as_pyplot_figure()\n",
    "    #plt.show()\n",
    "    \n",
    "    # shap interpretability\n",
    "    \n",
    "#important feature list\n",
    "print ('important_features: ', list(important_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_dataset_exp_full = df\n",
    "\n",
    "#Taking important features\n",
    "prepared_dataset_exp = df[list(important_features)+['albMCI']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(prepared_dataset_exp.drop(columns=['albMCI']).values, prepared_dataset_exp['albMCI'].values, test_size=0.2, random_state=123, stratify=prepared_dataset_exp['albMCI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn import tree\n",
    "from six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus, joblib\n",
    "from svglib.svglib import svg2rlg\n",
    "from reportlab.graphics import renderPDF, renderPM\n",
    "\n",
    "feature_names = prepared_dataset_exp.drop(columns=['albMCI']).columns\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "clf.fit(X_train, y_train) \n",
    "print(clf.score(X_test, y_test))\n",
    "y_pred = clf.predict(X_test)\n",
    "print (classification_report(y_test, y_pred))\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names=feature_names, \n",
    "               class_names=['No Dementia', 'Minimal or Mild Dementia'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('heliad_31-3.csv')\n",
    "df['MMSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(columns=['G21'])\n",
    "y=df['G21']\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# RandomForest Model\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=422)\n",
    "\n",
    "# Store predictions and actual values\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Perform LOO-CV\n",
    "for train_index, test_index in loo.split(X_scaled,y):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the left-out sample\n",
    "    pred = rf.predict(X_test)\n",
    "    \n",
    "    # Store results\n",
    "    y_pred.append(pred)\n",
    "    y_true.append(y_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Leave-One-Out CV Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importance = rf.feature_importances_\n",
    "\n",
    "# Get top 10 features\n",
    "top_10_indices = np.argsort(feature_importance)[::-1][:10]\n",
    "top_10_features = feature_importance[top_10_indices]\n",
    "\n",
    "print(\"Top 10 Feature Indices:\", top_10_indices)\n",
    "print(\"Top 10 Feature Importances:\", top_10_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [613, 596, 533,288,94,614,352,227,382,568]  # Example indices\n",
    "column_names = X.columns[indices].tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [74,\n",
    " 157,\n",
    " 197,\n",
    " 204,\n",
    " 288,\n",
    " 297,\n",
    " 334,\n",
    " 347,\n",
    " 352,\n",
    " 362,\n",
    " 373,\n",
    " 377,\n",
    " 404,\n",
    " 533,\n",
    " 596,\n",
    " 604,\n",
    " 609,\n",
    " 613,\n",
    " 614,\n",
    " 615]  # Example indices\n",
    "column_names = X.columns[indices].tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Selection\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split,RepeatedKFold,StratifiedShuffleSplit\n",
    "df=pd.read_csv('albion_31-3.csv')\n",
    "df=df.drop(columns=['ID','DIAGNOSIS'])\n",
    "df = df.replace({',': '.'}, regex=True)\n",
    "\n",
    "def convert_to_float(val):\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return val  # Return the value as-is if it can't be converted to float\n",
    "\n",
    "# Apply the function element-wise to the DataFrame\n",
    "df = df.applymap(convert_to_float)\n",
    "X=df.drop(columns=['Amyloid'])\n",
    "y=df['Amyloid']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "from collections import Counter\n",
    "Counts=Counter()\n",
    "X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "for i in range(0,10000):\n",
    "    random_state=i\n",
    "    metas=list(X.columns)\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "    for train_index, test_index in split.split(X,y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        class ArgsClass:\n",
    "            def __init__(self,alpha=0.005,k=10):\n",
    "                self.k=k\n",
    "                \n",
    "                self.alpha=alpha\n",
    "        args=ArgsClass(alpha=0.005,k=10)\n",
    "        sel=Selection(\"lasso\",args)\n",
    "        sel.fit(X_train,y_train)\n",
    "        mb=sel.mb_\n",
    "        Counts=Counter(mb)+Counts\n",
    "\n",
    "Counts_metas=pd.DataFrame(Counts.most_common(),columns=['metas_index','counts'])\n",
    "Counts_metas['metas'] = Counts_metas.apply(lambda a:metas[a['metas_index']],axis=1)\n",
    "selected_feature_index=list(Counts_metas['metas_index'][:10])\n",
    "print(selected_feature_index)\n",
    "s=[]\n",
    "s1=[]\n",
    "model = RF(n_estimators=200,random_state=1231106)\n",
    "model.fit(X_train[:,Counts_metas['metas_index'][0:10]], y_train)\n",
    "pred = model.predict(X_test[:,Counts_metas['metas_index'][0:10]])\n",
    "prob = model.predict_proba(X_test[:,Counts_metas['metas_index'][0:10]])\n",
    "print(\"ytest,pred:\",y_test,pred)\n",
    "s.append(f1_score(y_test, pred))\n",
    "print(\"ytest,prob:\",y_test,prob[:,-1])\n",
    "s1.append(roc_auc_score(y_test, prob[:,-1]))\n",
    "print(\"f1 score:\", s)\n",
    "print(\"auroc score:\", s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp=model.feature_importances_\n",
    "feature_importances=pd.concat([pd.DataFrame(Counts_metas['metas'][0:20]),pd.DataFrame(imp)],axis=1)\n",
    "feature_importances.columns=['feature_name','importance_score']\n",
    "feature_importances['feature_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(X,columns=df.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices =[609,618,607,617,619,623,262,56,112,388,610,621,240,157,39,377,213,613,198,627,120,524,401,9,250,628,620]\n",
    "column_names = X.columns[indices].tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('heliad_31-3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['G21', 'MMSE', 'G17psy', 'G3', 'C12','E4a','Oxy 9-oxoODE','C1','H3','Arachidic acid (C20:0)']\n",
    "\n",
    "# Keep only the specified columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "print(df.head())  # Display first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(columns=['G21'])\n",
    "y=df['G21']\n",
    "Xtrain,Xtest,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y)\n",
    "scaler = MinMaxScaler()\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xtest=scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=[]\n",
    "s1=[]\n",
    "model.fit(Xtrain, y_train)\n",
    "pred = model.predict(Xtest)\n",
    "prob = model.predict_proba(Xtest)\n",
    "print(\"ytest,pred:\",y_test,pred)\n",
    "s.append(f1_score(y_test, pred))\n",
    "print(\"ytest,prob:\",y_test,prob[:,-1])\n",
    "s1.append(roc_auc_score(y_test, prob[:,-1]))\n",
    "print(\"f1 score:\", s)\n",
    "print(\"auroc score:\", s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df.shape\n",
    "df = df.replace({',': '.'}, regex=True)\n",
    "\n",
    "def convert_to_float(val):\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return val  # Return the value as-is if it can't be converted to float\n",
    "\n",
    "# Apply the function element-wise to the DataFrame\n",
    "df = df.applymap(convert_to_float)\n",
    "#df=df.drop(columns=['Sample','Client CODE'])\n",
    "#df=df.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_before_or_equal_sex = df.loc[:, :'SEX'].columns.tolist()\n",
    "\n",
    "# If 'diagnosis' exists and is not already included, add it\n",
    "if 'DIAGNOSIS' in df.columns and 'DIAGNOSIS' not in cols_before_or_equal_sex:\n",
    "    cols_before_or_equal_sex.append('DIAGNOSIS')\n",
    "\n",
    "# Keep only the selected columns\n",
    "df = df[cols_before_or_equal_sex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('albion_31-3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_values = [\n",
    "    0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0,\n",
    "    1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "    1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n",
    "    1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0,\n",
    "    1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0,\n",
    "    0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n",
    "    1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n",
    "    1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "    1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n",
    "    1.0, 1.0, 0.0, 1.0, 0.0, 0.0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MCI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import KFold,RepeatedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler,RobustScaler,StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
    "df=df.drop(columns=['ID','Amyloid'])\n",
    "X=df.drop(columns=['DIAGNOSIS'])\n",
    "y=df['DIAGNOSIS']\n",
    "\n",
    "# Initialize cross-validation\n",
    "kf = RepeatedKFold(n_splits=10,n_repeats=10, random_state=1)\n",
    "scaler=MinMaxScaler()\n",
    "X=scaler.fit_transform(X)\n",
    "kf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "feature_counts = np.zeros(X.shape[1])  # To store selection counts\n",
    "selected_feature_sets = []  # Store selected features per fold\n",
    "all_coefficients = []\n",
    "auc_scores = []  # Store AUC scores\n",
    "\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit Lasso model\n",
    "    lasso = Lasso(alpha=0.005)  # Adjust alpha as needed\n",
    "    selector = SelectFromModel(lasso, prefit=False)\n",
    "    selector.fit(X_train, y_train)\n",
    "    coefficients = selector.estimator_.coef_\n",
    "    \n",
    "    # Store the coefficients for this fold\n",
    "    all_coefficients.append(coefficients)\n",
    "    # Get selected features and update count\n",
    "    selected_features = selector.get_support()\n",
    "    feature_counts += selected_features\n",
    "    selected_feature_sets.append(selected_features)\n",
    "\n",
    "# Determine features selected in more than 50% of folds\n",
    "selected_feature_mask = feature_counts > (kf.get_n_splits() / 2)\n",
    "#selected_feature_mask = feature_counts > (kf.get_n_splits() * 0.8)\n",
    "\n",
    "X_selected = X[:,selected_feature_mask]\n",
    "all_coefficients = np.array(all_coefficients)\n",
    "\n",
    "# Now, all_coefficients has shape (100, n_features)\n",
    "# To get the average coefficients across all folds and repeats\n",
    "average_coefficients = np.mean(all_coefficients, axis=0)\n",
    "\n",
    "# Print the average coefficients\n",
    "print(\"Average Lasso Coefficients across all folds and repeats:\", average_coefficients)\n",
    "# Train Random Forest on selected features with 10-fold CV\n",
    "\n",
    "auc_scores = []\n",
    "for train_index, test_index in kf.split(X_selected):\n",
    "    X_train, X_test = X_selected[train_index], X_selected[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate AUC-ROC\n",
    "    y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "feature_names = [f'Feature {i}' for i in range(X.shape[1])]\n",
    "feature_selection_results = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Times Selected': feature_counts\n",
    "})\n",
    "\n",
    "# Print feature selection results\n",
    "print(feature_selection_results.sort_values(by='Times Selected', ascending=False))\n",
    "\n",
    "# Print mean AUC-ROC\n",
    "print(f'Mean AUC-ROC: {np.mean(auc_scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Average Coefficient': average_coefficients\n",
    "})\n",
    "\n",
    "# Sort by absolute value of the coefficient to see which features have the biggest impact\n",
    "coefficients_df['Abs Coefficient'] = coefficients_df['Average Coefficient'].abs()\n",
    "coefficients_df = coefficients_df.sort_values(by='Abs Coefficient', ascending=False)\n",
    "\n",
    "# Display the coefficients with their corresponding feature names\n",
    "print(coefficients_df[['Feature', 'Average Coefficient']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_df.to_csv('ALBIONlasso_coeffs_featureSELECTION.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_results.sort_values(by='Times Selected',ascending=False)[:45]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('albion_31-3.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(columns=['DIAGNOSIS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indices = feature_selection_results.sort_values(by='Times Selected', ascending=False).head(35).index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = X.iloc[:, indices]\n",
    "\n",
    "# Now, selected_features will contain the columns from X corresponding to the indices\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns = df.columns[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['DIAGNOSIS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[609, 618, 607, 617, 619, 623, 262, 56, 112, 388, 610, 621, 240, 157, 39, 377, 213, 613, 198, 627, 120, 524, 401, 9, 250, 628, 620\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "column_names = df.columns[indices].tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('albionAPRIL_ONLYmetabs-better.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split,RepeatedKFold,StratifiedShuffleSplit\n",
    "X=df.drop(columns=['albMCI'])\n",
    "y=df['albMCI']\n",
    "\n",
    "# Initialize cross-validation\n",
    "scaler=MinMaxScaler()\n",
    "#X_selected=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('output.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV\n",
    "\n",
    "# Define the list of columns you want to keep\n",
    "columns_to_keep = [\n",
    "    'TG(49:1) [NL-16:1]', 'EXECUTIVE', 'PC(35:2) (a\b)', 'MH8', 'LANGUAGE', 'MMSE',\n",
    "    'HADS_A1', 'E4a', 'FA 18:1', 'd-Xylitol', 'C5:0', 'Cer(d18:1/20:0)', 'PG(36:2)',\n",
    "    'Horm pregnenolone sulfate', 'MH7', 'PC(40:8)', 'SM(43:2) (a  b c)', 'C5:1',\n",
    "    'SMOKING', 'Cer(d19:1/24:0)', 'CE(20:1)', 'FA(12:2)-3OH', 'Oxy 15(s)-HETE',\n",
    "    'C5-M-DC', 'Cer(d19:1/24:1)', 'SM(d18:1/18:0)/SM(d16:1/20:0)', 'BA LCA',\n",
    "    'Cer(d18:1/21:0)', 'FA 18:2', 'SM(43:1)', 'L-Alanine', 'LPC 20:4 sn1',\n",
    "    'FamilyHistory', 'FA(8:0)-2OH', 'PC(28:0)', 'Cer(m18:0/24:0)', 'LPC 20:1 sn1',\n",
    "    'C18:2', 'LPE 20:4 sn1', 'PC(P-15:0/20:4) (a\b)', 'SEX'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame\n",
    "df_filtered = df[columns_to_keep]\n",
    "\n",
    "# Optionally save the filtered CSV\n",
    "df_filtered.to_csv(\"new_albion_columnsJUNE2025.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('new_albion_columnsJUNE2025_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both CSV files\n",
    "df1 = pd.read_csv(\"albionAPRIL.csv\")  # Replace with actual file names\n",
    "df2 = pd.read_csv(\"new_albion_columnsJUNE2025.csv\")\n",
    "\n",
    "# Find common columns\n",
    "common_columns = list(set(df1.columns) & set(df2.columns))\n",
    "\n",
    "# Print them\n",
    "print(\"Common columns:\")\n",
    "for col in common_columns:\n",
    "    print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_values = [\n",
    "    0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0,\n",
    "    1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "    1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n",
    "    1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0,\n",
    "    1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0,\n",
    "    0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n",
    "    1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n",
    "    1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "    1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n",
    "    1.0, 1.0, 0.0, 1.0, 0.0, 0.0\n",
    "]\n",
    "\n",
    "# Step 3: Attach as a new column\n",
    "df2[\"MCI\"] = mci_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['MCI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('new_albion_columnsJUNE2025_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=4211)\n",
    "import numpy as np\n",
    "\n",
    "# Train Random Forest on selected features with 10-fold CV\n",
    "auc_scores = []\n",
    "for train_index, test_index in kf.split(X_selected,y):\n",
    "    X_train, X_test = X_selected[train_index], X_selected[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=41211)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate AUC-ROC\n",
    "    y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, f1_score\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Print feature selection results\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "# Print mean AUC-ROC\n",
    "print(f'Mean AUC-ROC: {np.mean(auc_scores):.4f}')\n",
    "s=[]\n",
    "s1=[]\n",
    "model2 = RandomForestClassifier(n_estimators=200,random_state=41211)\n",
    "\n",
    "model2.fit(X_train, y_train)\n",
    "pred = model2.predict(X_test)\n",
    "prob = model2.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "\n",
    "# Calculate base ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Bootstrapping for confidence intervals\n",
    "n_bootstraps = 1000\n",
    "auc_scores = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    indices = resample(range(len(y_test)), replace=True)  # Sample with replacement\n",
    "    if len(set(y_test.iloc[indices])) < 2:  # Ensure both classes are present\n",
    "        continue\n",
    "    fpr_i, tpr_i, _ = roc_curve(y_test.iloc[indices], prob[indices])\n",
    "    auc_scores.append(auc(fpr_i, tpr_i))\n",
    "\n",
    "# Compute 95% confidence interval\n",
    "lower_bound = np.percentile(auc_scores, 2.5)\n",
    "upper_bound = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "# Plot ROC curve with 95% CI\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.fill_between(fpr, np.percentile(tpr, 2.5, axis=0), np.percentile(tpr, 97.5, axis=0),\n",
    "                 color=\"blue\", alpha=0.2, label=f\"95% CI ({lower_bound:.2f} - {upper_bound:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")  # Random classifier\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve with 95% Confidence Interval\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print scores\n",
    "print(\"F1 Score:\", f1_score(y_test, pred))\n",
    "print(\"AUC-ROC Score:\", roc_auc_score(y_test, prob))\n",
    "print(f\"95% AUC Confidence Interval: ({lower_bound:.4f}, {upper_bound:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "\n",
    "X=df.drop(columns=['MCI'])\n",
    "y=df['MCI']\n",
    "# Assuming X_selected and y are already defined\n",
    "s = []\n",
    "s1 = []\n",
    "n_bootstraps = 1000  # Bootstrapping for confidence intervals\n",
    "\n",
    "# Initialize model\n",
    "model2 = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "    \n",
    "        eval_metric='AUC',\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "# 10-Fold Cross Validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit model\n",
    "    model2.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions and probabilities\n",
    "    pred = model2.predict(X_test)\n",
    "    prob = model2.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Bootstrapping for AUC confidence intervals\n",
    "    auc_scores = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        indices = resample(range(len(y_test)), replace=True)  # Sample with replacement\n",
    "        if len(set(y_test.iloc[indices])) < 2:  # Ensure both classes are present\n",
    "            continue\n",
    "        fpr_i, tpr_i, _ = roc_curve(y_test.iloc[indices], prob[indices])\n",
    "        auc_scores.append(auc(fpr_i, tpr_i))\n",
    "\n",
    "    # Compute 95% confidence interval\n",
    "    lower_bound = np.percentile(auc_scores, 2.5)\n",
    "    upper_bound = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "    # Plot ROC curve for each fold\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color=\"blue\", label=f\"Fold {fold} ROC curve (AUC = {roc_auc:.2f})\")\n",
    "    plt.fill_between(fpr, np.percentile(tpr, 2.5, axis=0), np.percentile(tpr, 97.5, axis=0),\n",
    "                     color=\"blue\", alpha=0.2, label=f\"95% CI ({lower_bound:.2f} - {upper_bound:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")  # Random classifier\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve - Fold {fold} with 95% CI\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Append F1 score and AUC to lists\n",
    "    s.append(f1_score(y_test, pred))\n",
    "    s1.append(roc_auc_score(y_test, prob))\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Print overall performance\n",
    "print(\"F1 Scores across all folds:\", s)\n",
    "print(\"AUC-ROC Scores across all folds:\", s1)\n",
    "print(f'Mean AUC-ROC: {np.mean(s1):.4f}')\n",
    "print(f'Mean F1 Score: {np.mean(s):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########HELIAD WITH SAME COLUMNS############################\n",
    "\n",
    "\n",
    "df=pd.read_csv('heliad_31-3.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metabolites SM(41:1) AND PG(36:2) NOT IN HELIAD\n",
    "columns_to_keep =['LPC(P-18:0)', 'd-Xylitol', 'SM(d18:2/20:0)', 'CE(16:2)', 'C5-M-DC', 'dhCer(d18:0/22:0)', 'L-Phenylalanine', 'd-Arabitol', 'TG(53:2) [NL-17:1]', 'Oxy 13-oxoODE', 'Methyl linolenate (C18:3n3 [cis-9,12,15])', 'Threonic acid', 'Glutamic acid', 'LPE(P-18:1)', 'BA CA', 'PC(33:0) (a\\x08)', 'BA GCA', 'Oxy 9-oxoODE', 'FA 20:1', 'FA 20:3-iso2', 'Oxy 12(13)-DiHOME', 'Oxy 15(16)-EpODE', 'PC(40:5) (a\\x08)', 'PE(P-20:0/18:2)', 'L-Threonine', 'BA CDCA', 'FA(12:2)-3OH', 'LPE(P-18:0)', 'SM(40:3) (a\\x08)', 'Xylonic acid', 'FA(10:1)-3OH', 'Myo-Inositol', 'LPC(22:0) (a\\x08)', 'SM(43:1)', 'PE(P-18:1/18:1) (a\\x08)', 'Hex2Cer(d18:1/16:0)', 'LPE 18:1 sn1', 'FA(6:0)-3OH', 'PE(P-16:0/22:4)', 'LPC 20:1 sn1', 'TG(54:3) [NL-18:1]', 'PE(P-20:0/20:4)', 'PC(18:0_22:6)'\n",
    ",'Amyloid']\n",
    "\n",
    "\n",
    "\n",
    "# Keep only the specified columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "print(df.head())  # Display first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('amyloid_selected_feats_OVER50_ONLY_METABS.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({',': '.'}, regex=True)\n",
    "\n",
    "def convert_to_float(val):\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return val  # Return the value as-is if it can't be converted to float\n",
    "\n",
    "# Apply the function element-wise to the DataFrame\n",
    "df = df.applymap(convert_to_float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('heliadAPRIL_ONLY_METABS.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=421151)\n",
    "import numpy as np\n",
    "\n",
    "# Train Random Forest on selected features with 10-fold CV\n",
    "auc_scores = []\n",
    "fold_number = 1\n",
    "for train_index, test_index in kf.split(X_selected):\n",
    "    X_train, X_test = X_selected[train_index], X_selected[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=200)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate AUC-ROC\n",
    "    y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    auc_scores.append(auc)\n",
    "    \n",
    "\n",
    "\n",
    "# Print feature selection results\n",
    "\n",
    "# Print mean AUC-ROC\n",
    "print(f'Mean AUC-ROC: {np.mean(auc_scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "# Assuming X_selected and y are already defined\n",
    "s = []\n",
    "s1 = []\n",
    "n_bootstraps = 1000  # Bootstrapping for confidence intervals\n",
    "\n",
    "# Initialize model\n",
    "model2 = CatBoostClassifier(\n",
    "        iterations=200,depth=4,learning_rate=0.03,\n",
    "    \n",
    "        eval_metric='AUC',\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "# 10-Fold Cross Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit model\n",
    "    model2.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions and probabilities\n",
    "    pred = model2.predict(X_test)\n",
    "    prob = model2.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Bootstrapping for AUC confidence intervals\n",
    "    auc_scores = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        indices = resample(range(len(y_test)), replace=True)  # Sample with replacement\n",
    "        if len(set(y_test.iloc[indices])) < 2:  # Ensure both classes are present\n",
    "            continue\n",
    "        fpr_i, tpr_i, _ = roc_curve(y_test.iloc[indices], prob[indices])\n",
    "        auc_scores.append(auc(fpr_i, tpr_i))\n",
    "\n",
    "    # Compute 95% confidence interval\n",
    "    lower_bound = np.percentile(auc_scores, 2.5)\n",
    "    upper_bound = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "    # Plot ROC curve for each fold\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color=\"blue\", label=f\"Fold {fold} ROC curve (AUC = {roc_auc:.2f})\")\n",
    "    plt.fill_between(fpr, np.percentile(tpr, 2.5, axis=0), np.percentile(tpr, 97.5, axis=0),\n",
    "                     color=\"blue\", alpha=0.2, label=f\"95% CI ({lower_bound:.2f} - {upper_bound:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")  # Random classifier\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve - Fold {fold} with 95% CI\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Append F1 score and AUC to lists\n",
    "    s.append(f1_score(y_test, pred))\n",
    "    s1.append(roc_auc_score(y_test, prob))\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Print overall performance\n",
    "print(\"F1 Scores across all folds:\", s)\n",
    "print(\"AUC-ROC Scores across all folds:\", s1)\n",
    "print(f'Mean AUC-ROC: {np.mean(s1):.4f}')\n",
    "print(f'Mean F1 Score: {np.mean(s):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv('amyloid_selected_feats.csv')  # Your main dataset\n",
    "df2 = pd.read_csv('amyloid_selected_feats_OVER50_ONLY_METABS.csv')  # Dataset with updated values\n",
    "\n",
    "# Find shared column names\n",
    "shared_columns = df1.columns.intersection(df2.columns)\n",
    "\n",
    "# Replace only the shared columns in df1 with values from df2\n",
    "df1[shared_columns] = df2[shared_columns]\n",
    "\n",
    "# Save or use the updated dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('new_albion_columnsJUNE2025_2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_values = [\n",
    "    0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0,\n",
    "    1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "    1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n",
    "    1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0,\n",
    "    1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0,\n",
    "    0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n",
    "    1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n",
    "    1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    "    1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0,\n",
    "    0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n",
    "    1.0, 1.0, 0.0, 1.0, 0.0, 0.0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MCI2']=mci_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold,StratifiedGroupKFold,StratifiedKFold\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import f1_score, recall_score, roc_auc_score,confusion_matrix,accuracy_score,precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,RobustScaler,StandardScaler\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "df=pd.read_csv('new_heliad_columnsJUNE2025.csv')\n",
    "#df=df.drop(columns=['PG(36:2)'])\n",
    "df = df.replace({',': '.'}, regex=True)\n",
    "\n",
    "def convert_to_float(val):\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return val  # Return the value as-is if it can't be converted to float\n",
    "\n",
    "# Apply the function element-wise to the DataFrame\n",
    "df = df.applymap(convert_to_float)\n",
    "#df=pd.read_csv('albionAPRIL.csv')\n",
    "#df.rename(columns={'C1': 'Hypertension History', 'E4a': 'Going out to see friends','C12': 'Cancer History', 'H3': 'Sleep problems','C55': 'Antiepileptic drug medication', 'F2': 'Manages finances','C27':'SMOKING' }, inplace=True)\n",
    "#df=df.drop(columns=['DIAGNOSIS','MCI'])\n",
    "X=df.drop(columns=['G21'])\n",
    "y=df['G21']\n",
    "\n",
    "# Initialize cross-validation\n",
    "scaler=StandardScaler()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "rf=CatBoostClassifier(iterations=300)# 10-fold stratified cross-validation\n",
    "#cv = RepeatedStratifiedKFold(n_repeats=10,n_splits=10,random_state=2)\n",
    "import random\n",
    "# Store scores\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "roc_auc_scores = []\n",
    "acc_scores=[]# Manual CV loop\n",
    "prec_scores=[]\n",
    "# 10 x 10-fold stratified CV with different random states\n",
    "n_repeats = 10\n",
    "n_splits = 10\n",
    "\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_repeats=10,n_splits=10)\n",
    "\n",
    "for train_idx, test_idx in cv.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train)  # Fit only on train\n",
    "        X_test_scaled = scaler.transform(X_test)        # Transform test\n",
    "\n",
    "    # Fit the model\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = rf.predict(X_test_scaled)\n",
    "        y_proba = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # Scores\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "        recall_scores.append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "        roc_auc_scores.append(roc_auc_score(y_test, y_proba))\n",
    "        acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "        prec_scores.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "\n",
    "prec_scores.append(precision_score(y_test,y_pred,zero_division=0))\n",
    "# Print average scores\n",
    "print(\"Average F1 Score   :\", np.mean(f1_scores))\n",
    "print(\"Average Recall     :\", np.mean(recall_scores))\n",
    "print(\"Average ROC AUC    :\", np.mean(roc_auc_scores))\n",
    "print(\"Average Accuracy   :\", np.mean(acc_scores))\n",
    "print(\"Average Precision   :\", np.mean(prec_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heliad=pd.read_csv('heliad_31-3.csv')\n",
    "df_heliad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg36:2 not found\n",
    "columns_to_keep = [\n",
    "    'TG(49:1) [NL-16:1]', 'G3','G17psy','PC(35:2) (a)' , 'C1','MMSE','J16','E4a','FA 18:1','d-Xylitol','C5:0','Cer(d18:1/20:0)','Horm pregnenolone sulfate','C2','PC(40:8)','SM(43:2) (a b c)',\n",
    "    'C5:1','C27','Cer(d19:1/24:0)','CE(20:1)','FA(12:2)-3OH','Oxy 15(s)-HETE','C5-M-DC','Cer(d19:1/24:1)','SM(d18:1/18:0)/SM(d16:1/20:0)',\n",
    "    'BA LCA','Cer(d18:1/21:0)','FA 18:2','SM(43:1)','L-Alanine','LPC 20:4 sn1','FHdementia','FA(8:0)-2OH','PC(28:0)','Cer(m18:0/24:0)','LPC 20:1 sn1','C18:2','LPE 20:4 sn1','PC(P-15:0/20:4) (a)',\n",
    "    'G21','A11'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame\n",
    "df_filtered = df_heliad[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv('new_heliad_columnsJUNE2025.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='orange')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Random forest Feature Importances for HELIAD (new MCI)')\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "cell_text = [\n",
    "    [\"AUC\", \"0.77\"],\n",
    "    [\"F1\", \"0.71\"],\n",
    "    [\"Classif. Error\", \"28%\"],\n",
    "    ['Recall','0.71']\n",
    "]\n",
    "\n",
    "# Add the table to the plot (positioned in the upper right corner)\n",
    "plt.table(cellText=cell_text,\n",
    "          colWidths=[0.25]*2,\n",
    "          cellLoc='left',\n",
    "          colLabels=None,\n",
    "          loc='upper right',\n",
    "          bbox=[0.6, 0.4, 0.35, 0.25])  # [x, y, width, height]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df.sort_values(by='Importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "shap_values = rf.get_feature_importance(Pool(X, label=y), type='ShapValues')\n",
    "\n",
    "# For binary classification, shap_values has shape (n_samples, n_features + 1)\n",
    "# The last column is the expected value (base value)\n",
    "\n",
    "# Get mean absolute SHAP values for the positive class\n",
    "mean_abs_shap = np.abs(shap_values[:, :-1]).mean(axis=0)\n",
    "\n",
    "# Put into DataFrame for plotting\n",
    "shap_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'SHAP Importance': mean_abs_shap\n",
    "}).sort_values(by='SHAP Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(shap_df['Feature'], shap_df['SHAP Importance'], color='green')\n",
    "plt.xlabel('Mean |SHAP Value| (Positive Class)')\n",
    "plt.title('Feature Importance for Positive Class (SHAP)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns  # if X_train is a DataFrame\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort and display\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = pd.read_csv('helmet_MCI_Imp_INT.csv')\n",
    "target_df = pd.read_csv('heliadAPRIL.csv')\n",
    "\n",
    "# Select the column to copy (e.g., 'new_column')\n",
    "column_to_add = source_df['MCI']\n",
    "\n",
    "# Add the column to the target DataFrame\n",
    "# Make sure they have the same number of rows or align as needed\n",
    "target_df['DIAGNOSIS'] = column_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df['DIAGNOSIS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df=target_df.drop(columns=['G21'])\n",
    "target_df.to_csv('heliadAPRIL2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('albionAPRIL.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "df=pd.read_csv('albionAPRIL_AMYLOID.csv')\n",
    "df.shape\n",
    "df=df.drop(columns=['Amyloid'])\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "df.shape\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Simulate dataset ---\n",
    "X=df.drop(columns=['DIAGNOSIS'])\n",
    "y=df['DIAGNOSIS']\n",
    "\n",
    "# --- Randomly select 25 features ---\n",
    "#selected_features = random.sample(list(X.columns), 25)\n",
    "X_selected = X\n",
    "\n",
    "# --- KFold cross-validation setup ---\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=4132)\n",
    "\n",
    "# --- Store metrics ---\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "# --- Perform CV manually ---\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X_selected)):\n",
    "    X_train, X_test = X_selected.iloc[train_idx], X_selected.iloc[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=300, random_state=421)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute metrics for this fold\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    precisions.append(precision_score(y_test, y_pred))\n",
    "    recalls.append(recall_score(y_test, y_pred))\n",
    "    f1s.append(f1_score(y_test, y_pred))\n",
    "\n",
    "# --- Print average metrics ---\n",
    "print(\"Manual 10-Fold Cross-Validation Averages:\")\n",
    "print(f\"Accuracy:  {np.mean(accuracies):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f}\")\n",
    "print(f\"Recall:    {np.mean(recalls):.4f}\")\n",
    "print(f\"F1 Score:  {np.mean(f1s):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv(\"new_albion_columnsJUNE2025_2.csv\")\n",
    "df = df.replace({',': '.'}, regex=True)\n",
    "\n",
    "def convert_to_float(val):\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return val  # Return the value as-is if it can't be converted to float\n",
    "\n",
    "# Apply the function element-wise to the DataFrame\n",
    "df = df.applymap(convert_to_float)\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(20, 18))  # Adjust size for 43 features\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.title('Correlation Heatmap (43 Features)', fontsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If values vary too much, normalize (optional)\n",
    "# df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "# Plot heatmap of raw data\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(df, cmap='viridis', cbar=True)\n",
    "plt.title(\"Heatmap of Raw Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "\n",
    "z_scores = zscore(df)\n",
    "outliers = (np.abs(z_scores) > 3)\n",
    "\n",
    "# How many outliers per feature\n",
    "outlier_counts = outliers.sum(axis=0)\n",
    "print(\"Outliers per feature:\")\n",
    "print(outlier_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(data=df, orient='h')\n",
    "plt.title(\"Boxplot of Features (Outliers visible as dots)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv1 = pd.read_csv('albion_31-3.csv') \n",
    "csv1 = csv1.rename(columns={'ID': 'Client CODE'})\n",
    "  # 150 rows\n",
    "csv2 = pd.read_excel('albion_new.xlsx')    # 200 rows\n",
    "\n",
    "# Merge on 'clientcode' to keep only common ones\n",
    "merged = pd.merge(csv1, csv2[['Client CODE', 'MCI']], on='Client CODE', how='inner')\n",
    "\n",
    "# Now `merged` contains rows with clientcodes in both, and the mci column from second CSV\n",
    "# You can save or view it\n",
    "merged.to_csv('output.csv', index=False)\n",
    "print(merged.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
